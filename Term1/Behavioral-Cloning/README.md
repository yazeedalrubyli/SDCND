# Behaviorial Cloning

[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)

## Project Structure

| File                         | Description                                                             |
| ---------------------------- | ----------------------------------------------------------------------- |
| `model.py`                   | Convolutional Neural Network Pipeline to generate `Model.h5`            |
| `model.h5`                   | Trained model to predict steering wheel angle - Generated by `model.py` |
| `drive.py`                   | Drive the car using `model.h5` to predict the steering wheel angle      |

## Training Strategy

### Dataset
In simulator, there are 3 cameras atached to the car. These are the images taken by each camera:

| Postion | Size | Dimension | Color |
|---------|------|-----------|-------|
|  Left   | ~16K |  160x320  |  RGB  |
|  Center | ~16K |  160x320  |  RGB  |
|  Right  | ~16K |  160x320  |  RGB  |
|  **Total** | **~50k** |  **160x320**  |  **RGB**  |


> To get this amount of data, It takes about 30min of driving around the track.

### Augmentation
To get more data and to get rid of the bias in turning left because almost all the turns during the driving session are turning to the left, we flipped the images verticaly then multiply the each sterring wheel angle for flipped images by `-1`

```python

measurments = read_csv('data/driving_log.csv', usecols=[3]).values
C = measurments
L = measurments + 0.2
R = measurments - 0.2
measurments = np.concatenate((C, L, R, -C, -L, -R), axis=0)

```
> After this operation we got ~150k of images to train our network with

### 

## Model Architecture
[NVIDIA End to End Learning for Self-Driving Cars Paper](https://arxiv.org/abs/1604.07316)

## Results

<p align="center">
  <img src="Media/BehavioralCloning.gif" alt="Driving Autonomously"/>
  <br/>
  <a target="_blank" href="https://youtu.be/08jBeBCmbLE">Fully Autonomous Driving Video</a>
</p>

